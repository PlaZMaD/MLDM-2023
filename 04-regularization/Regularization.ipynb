{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PlaZMaD/MLDM-2024/blob/main/04-regularization/Regularization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyNyFPNLVxuA"
      },
      "source": [
        "# Boston housing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAiD9r1mV5ka"
      },
      "source": [
        "In this example we'll try to predict housing prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIxJDCLGXJ6W"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/selva86/datasets/refs/heads/master/BostonHousing.csv\"\n",
        "boston_data = pd.read_csv(url )"
      ],
      "metadata": {
        "id": "jbpe0iz6c2_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20pkSnXwX5fW"
      },
      "outputs": [],
      "source": [
        "data = boston_data\n",
        "data.columns = [col.upper() for col in data.columns]\n",
        "print(data.columns)\n",
        "X = data.drop('MEDV', axis=1)\n",
        "y = data['MEDV']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yuHygsmWAxW"
      },
      "source": [
        "Information about the dataset:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".. _boston_dataset:\n",
        "\n",
        "Boston house prices dataset\n",
        "---------------------------\n",
        "\n",
        "**Data Set Characteristics:**  \n",
        "\n",
        "    :Number of Instances: 506\n",
        "\n",
        "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
        "\n",
        "    :Attribute Information (in order):\n",
        "        - CRIM     per capita crime rate by town\n",
        "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "        - INDUS    proportion of non-retail business acres per town\n",
        "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "        - NOX      nitric oxides concentration (parts per 10 million)\n",
        "        - RM       average number of rooms per dwelling\n",
        "        - AGE      proportion of owner-occupied units built prior to 1940\n",
        "        - DIS      weighted distances to five Boston employment centres\n",
        "        - RAD      index of accessibility to radial highways\n",
        "        - TAX      full-value property-tax rate per $10,000\n",
        "        - PTRATIO  pupil-teacher ratio by town\n",
        "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\n",
        "        - LSTAT    % lower status of the population\n",
        "        - MEDV     Median value of owner-occupied homes in $1000's\n",
        "\n",
        "    :Missing Attribute Values: None\n",
        "\n",
        "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
        "\n",
        "This is a copy of UCI ML housing dataset.\n",
        "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
        "\n",
        "\n",
        "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
        "\n",
        "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
        "prices and the demand for clean air', J. Environ. Economics & Management,\n",
        "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
        "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
        "pages 244-261 of the latter.\n",
        "\n",
        "The Boston house-price data has been used in many machine learning papers that address regression\n",
        "problems.   \n",
        "     \n",
        ".. topic:: References\n",
        "\n",
        "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
        "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann."
      ],
      "metadata": {
        "id": "-cwTqOpUfLJ7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kYvpf7WWRaY"
      },
      "source": [
        "# Exploring the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDmkDPRnWfLh"
      },
      "source": [
        "Let's just see how the target depends on individual features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obHi592HaS7r"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "grid_size = int(np.ceil(X.shape[1]**0.5))\n",
        "\n",
        "for i, (name, x) in enumerate(X.items(), 1):\n",
        "  plt.subplot(grid_size, grid_size, i)\n",
        "  plt.scatter(x, y)\n",
        "  plt.xlabel(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11Dameq1X4rI"
      },
      "source": [
        "Let's start by trying a simple linear regression model on the features with the most obvious correlation with the target. We'll also scale the features manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNlSiXP_Ywjg"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KJJ8hzOiY85"
      },
      "outputs": [],
      "source": [
        "columns = [\"CRIM\", \"RM\", \"LSTAT\"]\n",
        "\n",
        "X_subset = X[columns]\n",
        "X_subset /= X_subset.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1Ws8w5-Zw82"
      },
      "outputs": [],
      "source": [
        "model = make_pipeline(\n",
        "    PolynomialFeatures(9, include_bias=False), # Can you calculate how many features this will result in? :)\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "model.fit(X_subset, y)\n",
        "print('mse = ', mean_squared_error(y, model.predict(X_subset)))\n",
        "\n",
        "plt.figure(figsize=(15, 4))\n",
        "for i, c in enumerate(columns, 1):\n",
        "  plt.subplot(1, len(columns), i)\n",
        "  plt.scatter(X[c], y, label='data')\n",
        "  plt.scatter(X[c], model.predict(X_subset), label='prediction')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4KzosykZHPD"
      },
      "source": [
        "# Splitting the data to train and validation parts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDABmqhgZRJ8"
      },
      "source": [
        "Looks like the fit from above is reasonable, right?\n",
        "\n",
        "In fact, we cannot know this yet: we fitted and predicted on the same data. Let's split our dataset to get a more reasonable estimate of the prediction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFTt4UrtfW8K"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1yoR0D2fZcM"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=50, random_state=39)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2vyMKoafgdd"
      },
      "outputs": [],
      "source": [
        "model = make_pipeline(\n",
        "    PolynomialFeatures(9, include_bias=False),\n",
        "    LinearRegression()\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train mse = ', mean_squared_error(y_train, model.predict(X_train)))\n",
        "print('test mse = ', mean_squared_error(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPdxrrPWZzA1"
      },
      "source": [
        "That's quite an error we have on the test set!\n",
        "\n",
        "Let's look at the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCIVdnmhkDUl"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 4))\n",
        "for i, c in enumerate(columns, 1):\n",
        "  plt.subplot(1, len(columns), i)\n",
        "  plt.scatter(X_test[c], y_test, label='data')\n",
        "  plt.scatter(X_test[c], model.predict(X_test), label='prediction')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JblHhLeKaq4y"
      },
      "source": [
        "That's because our parameter values at the solution are enormous:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2hRXCvqaYhw"
      },
      "outputs": [],
      "source": [
        "model.named_steps['linearregression'].coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47w2yMDTa3jw"
      },
      "source": [
        "# L2 regularization (ridge regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFqxu7Cia9PG"
      },
      "source": [
        "Let's regularize the solution!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJd9IDwkf0b_"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mPbo3d8gG-w"
      },
      "outputs": [],
      "source": [
        "model = make_pipeline(\n",
        "    PolynomialFeatures(9, include_bias=False),\n",
        "    Ridge(alpha=1.)\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train mse = ', mean_squared_error(y_train, model.predict(X_train)))\n",
        "print('test mse = ', mean_squared_error(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGKHmwLNk2wU"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 4))\n",
        "for i, c in enumerate(columns, 1):\n",
        "  plt.subplot(1, len(columns), i)\n",
        "  plt.scatter(X_test[c], y_test, label='data')\n",
        "  plt.scatter(X_test[c], model.predict(X_test), label='prediction')\n",
        "  plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxmdOpYJbN3V"
      },
      "outputs": [],
      "source": [
        "model.named_steps['ridge'].coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xHvB9VIbxrP"
      },
      "source": [
        "Now we'll study how losses and parameter values depend on the regularization power."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLn0yQe2nwOw"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjQwZ0g4jSkq"
      },
      "outputs": [],
      "source": [
        "reg_powers = np.logspace(-12, 5, 18 * 5, base=10)\n",
        "\n",
        "\n",
        "train_mse = []\n",
        "test_mse = []\n",
        "\n",
        "params = []\n",
        "\n",
        "for alpha in tqdm(reg_powers):\n",
        "  linear_model = Ridge(alpha=alpha)\n",
        "  model = make_pipeline(\n",
        "    PolynomialFeatures(9, include_bias=False),\n",
        "    linear_model\n",
        "  )\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  params.append(\n",
        "      np.append(linear_model.coef_,\n",
        "                linear_model.intercept_)\n",
        "  )\n",
        "\n",
        "  train_mse.append(mean_squared_error(y_train, model.predict(X_train)))\n",
        "  test_mse.append(mean_squared_error(y_test, model.predict(X_test)))\n",
        "\n",
        "params = np.array(params)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.plot(1. / reg_powers, train_mse, label='train')\n",
        "plt.plot(1. / reg_powers, test_mse, label='test')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('inverse regularization power')\n",
        "plt.legend()\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(1. / reg_powers, np.abs(params));\n",
        "plt.xlabel(\"inverse regularization power\")\n",
        "plt.ylabel(\"parameter magnitude\")\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f1cCilVcouK"
      },
      "source": [
        "# L1 regularization (lasso regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7S0rCyOBcvou"
      },
      "source": [
        "Here's a similar study with the Lasso regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JAYVacvm-CT"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eX6mAO5_oIvR"
      },
      "outputs": [],
      "source": [
        "reg_powers = np.logspace(-4, 1, 6 * 5, base=10)\n",
        "\n",
        "train_mse = []\n",
        "test_mse = []\n",
        "\n",
        "params = []\n",
        "\n",
        "for alpha in tqdm(reg_powers):\n",
        "  # Lasso doesn't have an analytic solution. Instead it\n",
        "  # utilizes an iterative procedure, which for small\n",
        "  # alpha values may take a while to converge.\n",
        "  linear_model = Lasso(alpha=alpha, max_iter=1000000)\n",
        "  model = make_pipeline(\n",
        "    PolynomialFeatures(9, include_bias=False),\n",
        "    linear_model\n",
        "  )\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  params.append(\n",
        "      np.append(linear_model.coef_,\n",
        "                linear_model.intercept_)\n",
        "  )\n",
        "\n",
        "  train_mse.append(mean_squared_error(y_train, model.predict(X_train)))\n",
        "  test_mse.append(mean_squared_error(y_test, model.predict(X_test)))\n",
        "\n",
        "params = np.array(params)\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "\n",
        "plt.plot(1. / reg_powers, train_mse, label='train')\n",
        "plt.plot(1. / reg_powers, test_mse, label='test')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('inverse regularization power')\n",
        "plt.legend()\n",
        "plt.xscale('log')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(1. / reg_powers, np.abs(params));\n",
        "plt.xlabel(\"inverse regularization power\")\n",
        "plt.ylabel(\"parameter magnitude\")\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(1. / reg_powers, np.isclose(params, 0.).sum(axis=1));\n",
        "plt.xlabel(\"inverse regularization power\")\n",
        "plt.ylabel(\"# zero parameters\")\n",
        "plt.xscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NotxiAE2gJZH"
      },
      "source": [
        "# Bonus. What features are the most powerful?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7p_2HQldxXV"
      },
      "source": [
        "Let's see what features are most powerful for a reasonably performing model (e.g. 1/alpha = 100) **(2 points)**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKNFwkHldX77"
      },
      "outputs": [],
      "source": [
        "model = make_pipeline(\n",
        "  PolynomialFeatures(9, include_bias=False),\n",
        "  Lasso(alpha=0.01, max_iter=1000000)\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print('train mse = ', mean_squared_error(y_train, model.predict(X_train)))\n",
        "print('test mse = ', mean_squared_error(y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqlQMxabgW9z"
      },
      "source": [
        "Some hints:\n",
        " - You can explore the feature names using `get_feature_names` method of the `PolynomialFeatures` class (plug the list of original feature names to get reasonable output).\n",
        " - `model.named_steps['polynomialfeatures']` to get the `PolynomialFeatures` preprocessor of our model.\n",
        " - `model.named_steps['lasso'].coef_` to get the parameters of the linear model\n",
        " - `np.argwhere` to find indices of non-zero elements of an array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKqrs_Hhe-sP"
      },
      "outputs": [],
      "source": [
        "<YOUR CODE>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}